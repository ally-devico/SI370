{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fake Vs Real News Predictative Modeling and Topic Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Ally Devico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES AND DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d185e5fb-ac9a-40a2-b563-d9aa1a77f94e",
    "_uuid": "1008fd2001c2b8485d2b4815813f90b722286c22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/allydevico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/allydevico/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/allydevico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/allydevico/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from numpy import hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv('../data/true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv('../data/fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a new column called \"true\" that keeps track of whether or not the row in the combined_df came from the true or fake dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df['true'] = 1\n",
    "fake_df['true'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([true_df, fake_df])\n",
    "combined_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed agency name from \"text\" and \"title\" columns. This is important as news agency has a really strong indicator of whether or not the article is fake. Thus, the homework assignment wants to push our boundaries and look into other things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_news_agency_name(text):\n",
    "    return re.sub(r\"Reuters|AP|New York Times|Washington Post|Business Insider|Atlantic|Fox News|National Review|Talking Points Memo|Buzzfeed News|Guardian|NPR|Vox|CNN|BBC|Bloomberg|Daily Mail\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['text'] = combined_df['text'].apply(remove_news_agency_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['title'] = combined_df['title'].apply(remove_news_agency_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made all strings lowercase in the entire dataframe. This is important as we dont want the same word that is capitalize one place and lowercase in another to be counted as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/p9hpbw597nb9frsx8622mh_c0000gn/T/ipykernel_69100/430303586.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  combined_df = combined_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed the categorical variables in \"subject\" column into numeric labels to that they can be more easily understood in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['subject'] = label_encoder.fit_transform(combined_df['subject'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIVE MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use TfidfVectorizer as it looks at many aspects of text including sentiment and common/rare words. It is possible that fake news articles have strong sentiments, while true articles remain modest. Similiarly, it is possible fake news articles contain terms that are uncommon in legitimate news sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X =extract.fit_transform(combined_df['text'])\n",
    "y = combined_df[\"true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9798440979955456\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Classifiers\n",
    "classifier_1 = LogisticRegression()\n",
    "classifier_2 = MultinomialNB()\n",
    "classifier_3 = RandomForestClassifier()\n",
    "\n",
    "# Create a Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "  ('rf', classifier_1),\n",
    "  ('dt', classifier_2),\n",
    "  ('svm', classifier_3)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the Voting Classifier\n",
    "voting_predictions = voting_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the Voting Classifier\n",
    "accuracy = accuracy_score(y_test, voting_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing cleaned versions of true_df and fake_df from cleaning we did in combined_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df_cleaned = combined_df[combined_df[\"true\"] == 1]\n",
    "fake_df_cleaned = combined_df[combined_df[\"true\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding most commonly occuring words in the text for true and fake news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said          99062\n",
       "trump         54700\n",
       "president     28177\n",
       "state         21025\n",
       "government    18846\n",
       "states        16652\n",
       "house         16640\n",
       "new           16391\n",
       "republican    16243\n",
       "united        15590\n",
       "people        15287\n",
       "year          14777\n",
       "told          14245\n",
       "party         12759\n",
       "washington    12571\n",
       "election      12306\n",
       "campaign      10636\n",
       "donald        10456\n",
       "security      10162\n",
       "percent       10012\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words='english', max_features=20)\n",
    "true_word_count = count_vec.fit_transform(true_df_cleaned[\"text\"])\n",
    "true_feature_names = count_vec.get_feature_names_out()\n",
    "true_word_count_df = pd.DataFrame(true_word_count.toarray(), columns=true_feature_names)\n",
    "true_word_counts = true_word_count_df.sum().sort_values(ascending=False)\n",
    "true_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trump        79307\n",
       "said         33763\n",
       "president    27719\n",
       "people       26570\n",
       "just         20511\n",
       "clinton      19173\n",
       "obama        18803\n",
       "like         18097\n",
       "donald       17671\n",
       "hillary      14124\n",
       "time         13844\n",
       "state        13463\n",
       "white        13190\n",
       "new          12824\n",
       "news         11816\n",
       "twitter      11724\n",
       "media        11704\n",
       "american     11319\n",
       "america      11185\n",
       "house        11113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_word_count = count_vec.fit_transform(fake_df_cleaned[\"text\"])\n",
    "fake_feature_names = count_vec.get_feature_names_out()\n",
    "fake_word_count_df = pd.DataFrame(fake_word_count.toarray(), columns=fake_feature_names)\n",
    "fake_word_counts = fake_word_count_df.sum().sort_values(ascending=False)\n",
    "fake_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in certain perctentages of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_true = true_df_cleaned['text']\n",
    "txt_fake = fake_df_cleaned['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'said']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0.5, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt_true)\n",
    "bag_of_words = count_vec.transform(txt_true)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"said\" and \"president\" appear in at least 50% of true news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'trump']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0.5, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt_fake)\n",
    "bag_of_words = count_vec.transform(txt_fake)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"said\" and \"trump\" appear in at least 50% of fake news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent words in true vs fake news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government', 'president', 'said', 'state', 'trump']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=5)\n",
    "\n",
    "count_train = count_vec.fit(txt_true)\n",
    "bag_of_words = count_vec.transform(txt_true)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most frequent words in true news articles are \"government\", \"president\", \"said\", 'state', and \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'people', 'president', 'said', 'trump']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allydevico/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=5)\n",
    "\n",
    "count_train = count_vec.fit(txt_fake)\n",
    "bag_of_words = count_vec.transform(txt_fake)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most frequent words in fake news articles are \"just\", \"people\", \"president\", 'said', and \"trump\"."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
